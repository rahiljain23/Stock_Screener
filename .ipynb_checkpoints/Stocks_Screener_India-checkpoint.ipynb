{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d612f9b-7f15-4355-9d39-579246e875fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08253bef-bd4a-45e3-9995-366119c851f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc234a34-3613-438e-b44d-6902ada78ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker_list = [\n",
    "#     \"RELIANCE.NS\", \"HDFCBANK.NS\", \"ICICIBANK.NS\", \"INFY.NS\", \"TCS.NS\",\n",
    "#     \"LT.NS\", \"ITC.NS\", \"KOTAKBANK.NS\", \"HINDUNILVR.NS\", \"AXISBANK.NS\",\n",
    "#     \"SBIN.NS\", \"BHARTIARTL.NS\", \"BAJFINANCE.NS\", \"ASIANPAINT.NS\", \"MARUTI.NS\",\n",
    "#     \"HCLTECH.NS\", \"SUNPHARMA.NS\", \"NTPC.NS\", \"TITAN.NS\", \"ONGC.NS\",\n",
    "#     \"ULTRACEMCO.NS\", \"POWERGRID.NS\", \"ADANIENT.NS\", \"JSWSTEEL.NS\", \"WIPRO.NS\",\n",
    "#     \"TATAMOTORS.NS\", \"INDUSINDBK.NS\", \"BAJAJFINSV.NS\", \"HINDALCO.NS\", \"GRASIM.NS\",\n",
    "#     \"TECHM.NS\", \"DIVISLAB.NS\", \"NESTLEIND.NS\", \"TATASTEEL.NS\", \"CIPLA.NS\",\n",
    "#     \"ADANIPORTS.NS\", \"DRREDDY.NS\", \"BPCL.NS\", \"SBILIFE.NS\", \"BRITANNIA.NS\",\n",
    "#     \"EICHERMOT.NS\", \"HEROMOTOCO.NS\", \"COALINDIA.NS\", \"BAJAJ-AUTO.NS\", \"HDFCLIFE.NS\",\n",
    "#     \"SHREECEM.NS\", \"APOLLOHOSP.NS\", \"M&M.NS\", \"ICICIPRULI.NS\", \"UPL.NS\", \"ABB.NS\", \"ACC.NS\", \"ADANIGREEN.NS\", \"ALKEM.NS\",\n",
    "#     \"AMBUJACEM.NS\", \"AUROPHARMA.NS\", \"BAJAJHLDNG.NS\", \"BANDHANBNK.NS\", \"BANKBARODA.NS\",\n",
    "#     \"BERGEPAINT.NS\", \"BIOCON.NS\", \"BOSCHLTD.NS\", \"CANBK.NS\", \"CHOLAFIN.NS\",\n",
    "#     \"COLPAL.NS\", \"DABUR.NS\", \"DIXON.NS\", \"GAIL.NS\", \"GODREJCP.NS\",\n",
    "#     \"HAVELLS.NS\", \"ICICIGI.NS\", \"ICICIPRULI.NS\", \"IDEA.NS\", \"IGL.NS\",\n",
    "#     \"INDIGO.NS\", \"INDUSTOWER.NS\", \"IOC.NS\", \"LICHSGFIN.NS\", \"MARICO.NS\", \"MOTHERSON.NS\", \"MRF.NS\", \"NHPC.NS\", \"NMDC.NS\",\n",
    "#     \"PAGEIND.NS\", \"PEL.NS\", \"PIDILITIND.NS\", \"PNB.NS\", \"RECLTD.NS\",\n",
    "#     \"SAIL.NS\", \"SIEMENS.NS\", \"SRF.NS\", \"TORNTPHARM.NS\", \"TRENT.NS\",\n",
    "#     \"TVSMOTOR.NS\", \"UBL.NS\", \"VEDL.NS\", \"VOLTAS.NS\", \"ZEEL.NS\"\n",
    "# ]\n",
    "\n",
    "# Input_date='2025-07-15'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4f14f66-205d-478a-8008-da5f1ea33a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = [\n",
    "    \"5PAISA.NS\",\n",
    "    \"AAVAS.NS\",\n",
    "    \"APTUS.NS\",\n",
    "    \"ARMANFIN.NS\",\n",
    "    \"ASIANPAINT.NS\",\n",
    "    \"BANDHANBNK.NS\",\n",
    "    \"DELTACORP.NS\",\n",
    "    \"HINDUNILVR.NS\",\n",
    "    \"INFY.NS\",\n",
    "    \"JIOFIN.NS\",   # Assumed ticker for Jio Financial Services â€” please verify\n",
    "    \"LALPATHLAB.NS\",\n",
    "    \"TATAELXSI.NS\",\n",
    "    \"TCS.NS\",\n",
    "    \"WIPRO.NS\",\n",
    "    \"SWIGGY.NS\",\n",
    "      \"^NSEI\",         # Nifty 50\n",
    "    \"^NSEBANK\",      # Nifty Bank\n",
    "    \"^NSEMDCP50\",    # Nifty Midcap 50\n",
    "    \"^CNX100\",       # Nifty 100\n",
    "    \"^CNXIT\",        # Nifty IT\n",
    "    \"^CNXFMCG\",      # Nifty FMCG\n",
    "    \"^CNXAUTO\",      # Nifty Auto\n",
    "    \"^CNXPHARMA\",    # Nifty Pharma\n",
    "    \"^CNXREALTY\",    # Nifty Realty\n",
    "    \"^CNXENERGY\"\n",
    "\n",
    "]\n",
    "Input_date='2025-07-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e18c624a-910e-409d-aee8-f2ecdc48cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stock(ticker):\n",
    "    Data=yf.download(tickers=ticker,period=\"5y\",interval=\"1d\",group_by=ticker)\n",
    "    \n",
    "    Data.columns = Data.columns.droplevel(0)\n",
    "    # Data.head(5)\n",
    "    Data=Data[0:-20]\n",
    "    ## Add Useful KPIs\n",
    "    \n",
    "    Data['RSI']=ta.rsi(Data[\"Close\"], length=14)\n",
    "    \n",
    "    info = yf.Ticker(ticker).info\n",
    "    Data['Sector']=info.get(\"sector\")\n",
    "    Data['Industry']=info.get(\"industry\")\n",
    "    \n",
    "    MACD=ta.macd(Data[\"Close\"], fast=12, slow=26, signal=9)\n",
    "    Data=pd.concat([Data,MACD],axis=1)\n",
    "    \n",
    "    Data.rename(columns={\n",
    "        \"MACD_12_26_9\": \"MACD_Line\",\n",
    "        \"MACDs_12_26_9\": \"MACD_Signal\",\n",
    "        \"MACDh_12_26_9\": \"MACD_Histogram\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    \n",
    "    Data[\"EMA_20\"] = ta.ema(Data[\"Close\"], length=20)\n",
    "    Data[\"EMA_50\"] = ta.ema(Data[\"Close\"], length=50)\n",
    "    \n",
    "    # Add Bollinger Bands (20-day default)\n",
    "    bbands = ta.bbands(Data[\"Close\"], length=20)\n",
    "    \n",
    "    # Combine with main DataFrame\n",
    "    Data = pd.concat([Data, bbands], axis=1)\n",
    "    \n",
    "    # Optional rename (if needed)\n",
    "    Data.rename(columns={\n",
    "        \"BBL_20_2.0\": \"BB_Lower\",\n",
    "        \"BBM_20_2.0\": \"BB_Middle\",\n",
    "        \"BBU_20_2.0\": \"BB_Upper\",\n",
    "        \"BBB_20_2.0\": \"BB_Bandwidth\",\n",
    "        \"BBP_20_2.0\": \"BB_Percent\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add ADX (default 14-period)\n",
    "    adx_df = ta.adx(Data[\"High\"], Data[\"Low\"], Data[\"Close\"], length=14)\n",
    "    \n",
    "    # Join it with main DataFrame\n",
    "    Data = pd.concat([Data, adx_df], axis=1)\n",
    "    \n",
    "    # Optional rename (for clarity)\n",
    "    Data.rename(columns={\n",
    "        \"ADX_14\": \"ADX\",\n",
    "        \"DMP_14\": \"DI_Plus\",\n",
    "        \"DMN_14\": \"DI_Minus\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add ATR (default length=14)\n",
    "    Data[\"ATR_14\"] = ta.atr(Data[\"High\"], Data[\"Low\"], Data[\"Close\"], length=14)\n",
    "    \n",
    "    # Add Stochastic Oscillator (default k=14, d=3)\n",
    "    stoch_df = ta.stoch(Data[\"High\"], Data[\"Low\"], Data[\"Close\"], k=14, d=3)\n",
    "    \n",
    "    # Combine with main DataFrame\n",
    "    Data = pd.concat([Data, stoch_df], axis=1)\n",
    "    \n",
    "    # Optional rename\n",
    "    Data.rename(columns={\n",
    "        \"STOCHk_14_3_3\": \"Stoch_%K\",\n",
    "        \"STOCHd_14_3_3\": \"Stoch_%D\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add CCI (default period = 20)\n",
    "    Data[\"CCI_20\"] = ta.cci(Data[\"High\"], Data[\"Low\"], Data[\"Close\"], length=20)\n",
    "\n",
    "\n",
    "    \n",
    "    Data['DayOfWeek'] = Data.index.dayofweek\n",
    "    Data['Month'] = Data.index.month\n",
    "    # Data.to_csv(\"test.csv\")\n",
    "    \n",
    "    Data.reset_index(inplace=True)\n",
    "    \n",
    "    ## Feature Engineering\n",
    "    \n",
    "    Data['Volume1']=Data['Volume'].shift(1)\n",
    "    Data['Volume_Inc']=(Data['Volume']-Data['Volume1'])/Data['Volume1']\n",
    "    \n",
    "    Data['EMA_20_Price']=(Data['Close']-Data['EMA_20'])/Data['EMA_20']\n",
    "    Data['EMA_50_Price']=(Data['Close']-Data['EMA_50'])/Data['EMA_50']\n",
    "    \n",
    "    Data['Stoch_k_D']=(Data['Stoch_%K']-Data['Stoch_%D'])\n",
    "    \n",
    "    Data['Max_Close_Next_10'] = Data['Close'].shift(-1).rolling(window=10, min_periods=1).max()\n",
    "    \n",
    "    # Calculate the percentage increase from the current close to the maximum future close\n",
    "    Data['Close_Inc_Max'] = (Data['Max_Close_Next_10'] - Data['Close']) / Data['Close']\n",
    "    \n",
    "    # Define the target variable\n",
    "    Data['Buy'] = Data['Close_Inc_Max'].apply(lambda x: 1 if x >= 0.05 else 0)\n",
    "    \n",
    "    ## Data Clean\n",
    "    \n",
    "    Clean_Data=Data[['RSI', 'MACD_Histogram','BB_Percent','ADX','ATR_14', 'Stoch_%K',\n",
    "           'Stoch_%D', 'CCI_20','Volume_Inc', 'EMA_20_Price',\n",
    "           'EMA_50_Price', 'Stoch_k_D', 'Buy','DayOfWeek','Month']]\n",
    "    \n",
    "    Clean_Data=Clean_Data.dropna(subset=['RSI', 'MACD_Histogram','BB_Percent','ADX','ATR_14', 'Stoch_%K',\n",
    "           'Stoch_%D', 'CCI_20','Volume_Inc', 'EMA_20_Price',\n",
    "           'EMA_50_Price','DayOfWeek','Month'])\n",
    "    \n",
    "    # Clean_Data.head(10)\n",
    "    \n",
    "    ## Visualize\n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    features = [\"RSI\", \"MACD_Histogram\", \"BB_Percent\", \"ADX\", \"ATR_14\", \n",
    "                \"Stoch_%K\", \"CCI_20\", \"EMA_20_Price\", \"Volume_Inc\",'DayOfWeek','Month']\n",
    "    \n",
    "    # for feature in features:\n",
    "    #     plt.figure(figsize=(6, 3))\n",
    "    #     sns.kdeplot(data=Clean_Data[Clean_Data[\"Buy\"] == 1], x=feature, label=\"Buy = 1\", shade=True)\n",
    "    #     sns.kdeplot(data=Clean_Data[Clean_Data[\"Buy\"] == 0], x=feature, label=\"Buy = 0\", shade=True)\n",
    "    #     plt.title(f\"{feature} vs Buy\")\n",
    "    #     plt.legend()\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "    \n",
    "    \n",
    "    # for feature in features:\n",
    "    #     plt.figure(figsize=(6, 3))\n",
    "    #     sns.boxplot(data=Clean_Data, x=\"Buy\", y=feature)\n",
    "    #     plt.title(f\"{feature} by Buy value\")\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "    \n",
    "    \n",
    "    Clean_Data.drop(columns=\"Volume_Inc\",inplace=True)\n",
    "    \n",
    "    # # Step 1: Compute correlation matrix\n",
    "    # corr_matrix = Clean_Data.corr()\n",
    "    \n",
    "    # # Step 2: Set up the heatmap\n",
    "    # plt.figure(figsize=(12, 8))\n",
    "    # sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "    # plt.title(\"Correlation Heatmap\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    # Clean_Data.head(5)\n",
    "    \n",
    "    ## Model Build\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(Clean_Data.drop(\"Buy\", axis=1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # X_scaled\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    y=Clean_Data['Buy']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # ### Logisitic\n",
    "    \n",
    "    # from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # model=LogisticRegression()\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y=model.predict(X_test)\n",
    "    # y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    # from sklearn.metrics import classification_report,roc_auc_score\n",
    "    # print(\"ðŸ”¹ Logistic Regression:\")\n",
    "    # print(classification_report(y_test, y))\n",
    "    # print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "    \n",
    "    # ### Random Forest\n",
    "    \n",
    "    # from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # model=RandomForestClassifier(random_state=42)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y=model.predict(X_test)\n",
    "    # y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    # feature_importances = model.feature_importances_\n",
    "    # features = Clean_Data.drop(\"Buy\", axis=1).columns\n",
    "    # # Combine into a DataFrame\n",
    "    # F_Imp = pd.DataFrame({\n",
    "    #     \"Feature\": features,\n",
    "    #     \"Importance\": feature_importances\n",
    "    # }).sort_values(\"Importance\", ascending=False)\n",
    "    # print(F_Imp)\n",
    "    # from sklearn.metrics import classification_report\n",
    "    # print(\"ðŸ”¹ RandomForestClassifier:\")\n",
    "    # print(classification_report(y_test, y))\n",
    "    # print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "    \n",
    "    ### XG Boost\n",
    "    \n",
    "    from xgboost import XGBClassifier\n",
    "    model=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y=model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    feature_importances = model.feature_importances_\n",
    "    features = Clean_Data.drop(\"Buy\", axis=1).columns\n",
    "    # Combine into a DataFrame\n",
    "    F_Imp = pd.DataFrame({\n",
    "        \"Feature\": features,\n",
    "        \"Importance\": feature_importances\n",
    "    }).sort_values(\"Importance\", ascending=False)\n",
    "    # print(F_Imp)\n",
    "    from sklearn.metrics import classification_report\n",
    "    # print(\"ðŸ”¹ XGBClassifier: \")\n",
    "    # print(classification_report(y_test, y))\n",
    "    # print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_Test = yf.download(tickers=ticker, period=\"60d\", interval=\"1d\", group_by=\"ticker\")\n",
    "    data_Test.columns = data_Test.columns.droplevel(0)\n",
    "    \n",
    "    data_Test['RSI']=ta.rsi(data_Test[\"Close\"], length=14)\n",
    "    \n",
    "    info = yf.Ticker(ticker).info\n",
    "    data_Test['Sector']=info.get(\"sector\")\n",
    "    data_Test['Industry']=info.get(\"industry\")\n",
    "    \n",
    "    MACD=ta.macd(data_Test[\"Close\"], fast=12, slow=26, signal=9)\n",
    "    data_Test=pd.concat([data_Test,MACD],axis=1)\n",
    "    \n",
    "    data_Test.rename(columns={\n",
    "        \"MACD_12_26_9\": \"MACD_Line\",\n",
    "        \"MACDs_12_26_9\": \"MACD_Signal\",\n",
    "        \"MACDh_12_26_9\": \"MACD_Histogram\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    \n",
    "    data_Test[\"EMA_20\"] = ta.ema(data_Test[\"Close\"], length=20)\n",
    "    data_Test[\"EMA_50\"] = ta.ema(data_Test[\"Close\"], length=50)\n",
    "    \n",
    "    # Add Bollinger Bands (20-day default)\n",
    "    bbands = ta.bbands(data_Test[\"Close\"], length=20)\n",
    "    \n",
    "    # Combine with main DataFrame\n",
    "    data_Test = pd.concat([data_Test, bbands], axis=1)\n",
    "    \n",
    "    # Optional rename (if needed)\n",
    "    data_Test.rename(columns={\n",
    "        \"BBL_20_2.0\": \"BB_Lower\",\n",
    "        \"BBM_20_2.0\": \"BB_Middle\",\n",
    "        \"BBU_20_2.0\": \"BB_Upper\",\n",
    "        \"BBB_20_2.0\": \"BB_Bandwidth\",\n",
    "        \"BBP_20_2.0\": \"BB_Percent\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add ADX (default 14-period)\n",
    "    adx_df = ta.adx(data_Test[\"High\"], data_Test[\"Low\"], data_Test[\"Close\"], length=14)\n",
    "    \n",
    "    # Join it with main DataFrame\n",
    "    data_Test = pd.concat([data_Test, adx_df], axis=1)\n",
    "    \n",
    "    # Optional rename (for clarity)\n",
    "    data_Test.rename(columns={\n",
    "        \"ADX_14\": \"ADX\",\n",
    "        \"DMP_14\": \"DI_Plus\",\n",
    "        \"DMN_14\": \"DI_Minus\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add ATR (default length=14)\n",
    "    data_Test[\"ATR_14\"] = ta.atr(data_Test[\"High\"], data_Test[\"Low\"], data_Test[\"Close\"], length=14)\n",
    "    \n",
    "    # Add Stochastic Oscillator (default k=14, d=3)\n",
    "    stoch_df = ta.stoch(data_Test[\"High\"], data_Test[\"Low\"], data_Test[\"Close\"], k=14, d=3)\n",
    "    \n",
    "    # Combine with main DataFrame\n",
    "    data_Test = pd.concat([data_Test, stoch_df], axis=1)\n",
    "    \n",
    "    # Optional rename\n",
    "    data_Test.rename(columns={\n",
    "        \"STOCHk_14_3_3\": \"Stoch_%K\",\n",
    "        \"STOCHd_14_3_3\": \"Stoch_%D\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add CCI (default period = 20)\n",
    "    data_Test[\"CCI_20\"] = ta.cci(data_Test[\"High\"], data_Test[\"Low\"], data_Test[\"Close\"], length=20)\n",
    "\n",
    "    data_Test['DayOfWeek'] = data_Test.index.dayofweek\n",
    "    data_Test['Month'] = data_Test.index.month\n",
    "    \n",
    "    data_Test['Volume1']=data_Test['Volume'].shift(1)\n",
    "    data_Test['Volume_Inc']=(data_Test['Volume']-data_Test['Volume1'])/data_Test['Volume1']\n",
    "    \n",
    "    data_Test['EMA_20_Price']=(data_Test['Close']-data_Test['EMA_20'])/data_Test['EMA_20']\n",
    "    data_Test['EMA_50_Price']=(data_Test['Close']-data_Test['EMA_50'])/data_Test['EMA_50']\n",
    "    \n",
    "    data_Test['Stoch_k_D']=(data_Test['Stoch_%K']-data_Test['Stoch_%D'])\n",
    "    \n",
    "    data_Test['Close1']=data_Test['Close'].shift(10)\n",
    "    data_Test['Close_Inc']=(data_Test['Close1']-data_Test['Close'])/data_Test['Close']\n",
    "    \n",
    "    data_Test.reset_index(inplace=True)\n",
    "    data_Test=data_Test[data_Test['Date']==Input_date]\n",
    "    \n",
    "    Clean_Data_Test=data_Test[['RSI', 'MACD_Histogram','BB_Percent','ADX','ATR_14', 'Stoch_%K',\n",
    "           'Stoch_%D', 'CCI_20','Volume_Inc', 'EMA_20_Price',\n",
    "           'EMA_50_Price', 'Stoch_k_D','DayOfWeek','Month']]\n",
    "    \n",
    "    # Step 1: Get feature columns (same as used during training)\n",
    "    features = ['RSI', 'MACD_Histogram', 'BB_Percent', 'ADX', 'ATR_14',\n",
    "                'Stoch_%K', 'Stoch_%D', 'CCI_20', 'EMA_20_Price', 'EMA_50_Price', 'Stoch_k_D','DayOfWeek','Month']\n",
    "    \n",
    "    # Step 2: Extract today's row (last row in the test table)\n",
    "    today_row = Clean_Data_Test[features].iloc[-1:]  # Keep it as DataFrame\n",
    "    \n",
    "    # Step 3: Apply the same scaler used during training\n",
    "    today_scaled = scaler.transform(today_row)\n",
    "    \n",
    "    # Step 4: Predict using the trained model\n",
    "    buy_prediction = model.predict(today_scaled)[0]\n",
    "    print(ticker)\n",
    "    buy_probability = model.predict_proba(today_scaled)[0][1]\n",
    "    \n",
    "    # Step 5: Print result\n",
    "    # print(\"ðŸ”® Buy Prediction:\", \"Yes\" if buy_prediction == 1 else \"No\")\n",
    "    # print(f\"ðŸ“Š Confidence: {buy_probability:.2%}\")\n",
    "    return {\n",
    "            \"Ticker\": ticker,\n",
    "            \"Buy\": \"Yes\" if buy_prediction == 1 else \"No\",\n",
    "            \"Confidence\": round(buy_probability * 100, 2)\n",
    "        }\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "164c2aa0-9c38-4583-b141-046ce538d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5PAISA.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAVAS.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APTUS.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMANFIN.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIANPAINT.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BANDHANBNK.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELTACORP.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HINDUNILVR.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFY.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIOFIN.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LALPATHLAB.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATAELXSI.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCS.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIPRO.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWIGGY.NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^NSEI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^NSEBANK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^NSEMDCP50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^CNX100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^CNXIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^CNXFMCG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^CNXAUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^CNXPHARMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^CNXREALTY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^CNXENERGY\n",
      "           Ticker  Buy  Confidence\n",
      "0       5PAISA.NS   No        4.84\n",
      "1        AAVAS.NS   No        4.37\n",
      "2        APTUS.NS   No        0.44\n",
      "3     ARMANFIN.NS  Yes       97.36\n",
      "4   ASIANPAINT.NS   No        0.01\n",
      "5   BANDHANBNK.NS   No        1.46\n",
      "6    DELTACORP.NS   No        0.12\n",
      "7   HINDUNILVR.NS   No        0.01\n",
      "8         INFY.NS   No       25.22\n",
      "9       JIOFIN.NS   No        0.31\n",
      "10  LALPATHLAB.NS   No        0.03\n",
      "11   TATAELXSI.NS   No        0.04\n",
      "12         TCS.NS  Yes       97.78\n",
      "13       WIPRO.NS   No       15.76\n",
      "14      SWIGGY.NS   No       35.05\n",
      "15          ^NSEI   No        0.47\n",
      "16       ^NSEBANK   No        0.02\n",
      "17     ^NSEMDCP50   No        0.05\n",
      "18        ^CNX100   No        0.04\n",
      "19         ^CNXIT  Yes       83.46\n",
      "20       ^CNXFMCG   No        0.01\n",
      "21       ^CNXAUTO   No        0.01\n",
      "22     ^CNXPHARMA   No        0.01\n",
      "23     ^CNXREALTY   No        2.30\n",
      "24     ^CNXENERGY   No        0.23\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    result = process_stock(ticker)\n",
    "    results.append(result)\n",
    "\n",
    "final_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "120ad13e-55d0-4222-acd3-2e3713be3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "filename = f\"{Input_date}_Screener_India.csv\"\n",
    "\n",
    "final_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a79f1-ecbc-40bc-a52a-a3564ed18394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
